CS229MachineLearning
ShervineAmidi&AfshineAmidi
1
SupervisedLearning
1.1
IntroductiontoSupervisedLearning
Givenasetofdatapoints
f
x
(1)
;:::;x
(
m
)
g
associatedtoasetofoutcomes
f
y
(1)
;:::;y
(
m
)
g
,we
wanttobuildaclassi˝erthatlearnshowtopredict
y
from
x
.
r
Typeofprediction
Thedi˙erenttypesofpredictivemodelsaresummedupinthetable
below:
Regression
Classi˝er
Outcome
Continuous
Class
Examples
Linearregression
Logisticregression,SVM,NaiveBayes
r
Typeofmodel
Thedi˙erentmodelsaresummedupinthetablebelow:
Discriminativemodel
Generativemodel
Goal
Directlyestimate
P
(
y
j
x
)
Estimate
P
(
x
j
y
)
todeduce
P
(
y
j
x
)
What'slearned
Decisionboundary
Probabilitydistributionsofthedata
Illustration
Examples
Regressions,SVMs
GDA,NaiveBayes
1.2
Notationsandgeneralconcepts
r
Hypothesis
Thehypothesisisnoted
h

andisthemodelthatwechoose.Foragiveninput
data
x
(
i
)
,themodelpredictionoutputis
h

(
x
(
i
)
)
.
r
Lossfunction
Alossfunctionisafunction
L
:(
z;y
)
2
R

Y
!
L
(
z;y
)
2
R
thattakesas
inputsthepredictedvalue
z
correspondingtotherealdatavalue
y
andoutputshowdi˙erent
theyare.Thecommonlossfunctionsaresummedupinthetablebelow:
Leastsquared
Logistic
Hinge
Cross-entropy
1
2
(
y

z
)
2
log(1+exp(

yz
))
max(0
;
1

yz
)


y
log(
z
)+(1

y
)log(1

z
)

Linearregression
Logisticregression
SVM
NeuralNetwork
r
Costfunction
Thecostfunction
J
iscommonlyusedtoassesstheperformanceofamodel,
andisde˝nedwiththelossfunction
L
asfollows:
J
(

)=
m
X
i
=1
L
(
h

(
x
(
i
)
)
;y
(
i
)
)
r
Gradientdescent
Bynoting

2
R
thelearningrate,theupdateruleforgradientdescent
isexpressedwiththelearningrateandthecostfunction
J
asfollows:

 



r
J
(

)
Remark:Stochasticgradientdescent(SGD)isupdatingtheparameterbasedoneachtraining
example,andbatchgradientdescentisonabatchoftrainingexamples.
r
Likelihood
Thelikelihoodofamodel
L
(

)
givenparameters

isusedto˝ndtheoptimal
parameters

throughmaximizingthelikelihood.Inpractice,weusethelog-likelihood
`
(

)=
log(
L
(

))
whichiseasiertooptimize.Wehave:

opt
=
argmax

L
(

)
r
Newton'salgorithm
TheNewton'salgorithmisanumericalmethodthat˝nds

such
that
`
0
(

)=0
.Itsupdateruleisasfollows:

 


`
0
(

)
`
00
(

)
Remark:themultidimensionalgeneralization,alsoknownastheNewton-Raphsonmethod,has
thefollowingupdaterule:

 



r
2

`
(

)


1
r

`
(

)
1.3
Linearmodels
1.3.1
Linearregression
Weassumeherethat
y
j
x
;

˘N
(

2
)
r
Normalequations
Bynoting
X
thematrixdesign,thevalueof

thatminimizesthecost
functionisaclosed-formsolutionsuchthat:

=(
X
T
X
)

1
X
T
y
StanfordUniversity
2
Fall2018
CS229MachineLearning
ShervineAmidi&AfshineAmidi
r
LMSalgorithm
Bynoting

thelearningrate,theupdateruleoftheLeastMeanSquares
(LMS)algorithmforatrainingsetof
m
datapoints,whichisalsoknownastheWidrow-Ho˙
learningrule,isasfollows:
8
j;
j
 

j
+

m
X
i
=1

y
(
i
)

h

(
x
(
i
)
)

x
(
i
)
j
Remark:theupdateruleisaparticularcaseofthegradientascent.
r
LWR
LocallyWeightedRegression,alsoknownasLWR,isavariantoflinearregressionthat
weightseachtrainingexampleinitscostfunctionby
w
(
i
)
(
x
)
,whichisde˝nedwithparameter
˝
2
R
as:
w
(
i
)
(
x
)=exp


(
x
(
i
)

x
)
2
2
˝
2

1.3.2
Classi˝cationandlogisticregression
r
Sigmoidfunction
Thesigmoidfunction
g
,alsoknownasthelogisticfunction,isde˝ned
asfollows:
8
z
2
R
;g
(
z
)=
1
1+
e

z
2
]0
;
1[
r
Logisticregression
Weassumeherethat
y
j
x
;

˘
Bernoulli
(
˚
)
.Wehavethefollowing
form:
˚
=
p
(
y
=1
j
x
;

)=
1
1+exp(


T
x
)
=
g
(

T
x
)
Remark:thereisnoclosedformsolutionforthecaseoflogisticregressions.
r
Softmaxregression
Asoftmaxregression,alsocalledamulticlasslogisticregression,is
usedtogeneralizelogisticregressionwhentherearemorethan2outcomeclasses.Byconvention,
weset

K
=0
,whichmakestheBernoulliparameter
˚
i
ofeachclass
i
equalto:
˚
i
=
exp(

T
i
x
)
K
X
j
=1
exp(

T
j
x
)
1.3.3
GeneralizedLinearModels
r
Exponentialfamily
Aclassofdistributionsissaidtobeintheexponentialfamilyifitcan
bewrittenintermsofanaturalparameter,alsocalledthecanonicalparameterorlinkfunction,

,asu˚cientstatistic
T
(
y
)
andalog-partitionfunction
a
(

)
asfollows:
p
(
y
;

)=
b
(
y
)exp(
T
(
y
)

a
(

))
Remark:wewilloftenhave
T
(
y
)=
y
.Also,
exp(

a
(

))
canbeseenasanormalizationparam-
eterthatwillmakesurethattheprobabilitiessumtoone.
Herearethemostcommonexponentialdistributionssummedupinthefollowingtable:
Distribution

T
(
y
)
a
(

)
b
(
y
)
Bernoulli
log

˚
1

˚

y
log(1+exp(

))
1
Gaussian

y

2
2
1
p
2
ˇ
exp


y
2
2

Poisson
log(

)
y
e

1
y
!
Geometric
log(1

˚
)
y
log

e

1

e


1
r
AssumptionsofGLMs
GeneralizedLinearModels(GLM)aimatpredictingarandom
variable
y
asafunctionfo
x
2
R
n
+1
andrelyonthefollowing3assumptions:
(1)
y
j
x
;

˘
ExpFamily
(

)
(2)
h

(
x
)=
E
[
y
j
x
;

]
(3)

=

T
x
Remark:ordinaryleastsquaresandlogisticregressionarespecialcasesofgeneralizedlinear
models.
1.4
SupportVectorMachines
Thegoalofsupportvectormachinesisto˝ndthelinethatmaximizestheminimumdistanceto
theline.
r
Optimalmarginclassi˝er
Theoptimalmarginclassi˝er
h
issuchthat:
h
(
x
)=
sign
(
w
T
x

b
)
where
(
w;b
)
2
R
n

R
isthesolutionofthefollowingoptimizationproblem:
min
1
2
jj
w
jj
2
suchthat
y
(
i
)
(
w
T
x
(
i
)

b
)
>
1
Remark:thelineisde˝nedas
w
T
x

b
=0
.
r
Hingeloss
ThehingelossisusedinthesettingofSVMsandisde˝nedasfollows:
L
(
z;y
)=[1

yz
]
+
=max(0
;
1

yz
)
StanfordUniversity
3
Fall2018
